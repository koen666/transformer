{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49d6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742e3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词编码--->这个词是什么\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__(vocab_size,d_model,padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f39c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#位置编码--->这个词在哪\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device).float().unsqueeze(1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df906fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,vacab_size,d_model,max_len,dropout,device):\n",
    "        super().__init__()\n",
    "        self.token_embedding=TokenEmbedding(vacab_size,d_model)\n",
    "        self.position_embedding=PositionalEmbedding(d_model,max_len,device)\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "        self.d_model=d_model\n",
    "    def forward(self,x):\n",
    "        tok_emb=self.token_embedding(x)\n",
    "        pos_emb=self.position_embedding(x)\n",
    "        return self.dropout(tok_emb+pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59201fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 token id:\n",
      "tensor([[2, 3, 1, 7],\n",
      "        [5, 1, 1, 9]], device='cuda:0')\n",
      "\n",
      "最终 embedding (带位置编码):\n",
      "tensor([[[-0.5092,  1.1175,  0.0351,  1.4108],\n",
      "         [-1.2399,  0.9976,  0.2590,  1.8516],\n",
      "         [ 1.0103, -0.4624,  0.0222,  1.1109],\n",
      "         [ 1.4013, -0.0000,  1.2351,  3.6217]],\n",
      "\n",
      "        [[ 0.3338,  1.7399,  0.0000,  1.1599],\n",
      "         [ 0.9350,  0.6003,  0.0111,  1.1111],\n",
      "         [ 1.0103, -0.4624,  0.0000,  1.1109],\n",
      "         [-0.3748, -0.6811,  1.2260,  0.4909]]], device='cuda:0',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n",
      "\n",
      "形状: torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    device = \"cuda\"\n",
    "    # 参数设置\n",
    "    vocab_size = 10  # 假设词表 10 个词\n",
    "    d_model = 4      # 每个词向量 4 维\n",
    "    max_len = 6      # 最大序列长度 6\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # 创建 TransformerEmbedding\n",
    "    embedding_layer = TransformerEmbedding(vocab_size, d_model, max_len, dropout, device)\n",
    "    embedding_layer = embedding_layer.to(device)\n",
    "    \n",
    "    # 构造输入 token id，batch_size=2, seq_len=4\n",
    "    tokens = torch.tensor([\n",
    "        [2, 3, 1, 7],\n",
    "        [5, 1, 1, 9]\n",
    "    ], dtype=torch.long,device=device)\n",
    "    \n",
    "    # 获取最终 embedding\n",
    "    output = embedding_layer(tokens)\n",
    "    \n",
    "    print(\"输入 token id:\")\n",
    "    print(tokens)\n",
    "    print(\"\\n最终 embedding (带位置编码):\")\n",
    "    print(output)\n",
    "    print(\"\\n形状:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
